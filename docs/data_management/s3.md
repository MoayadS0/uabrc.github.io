# Long-Term Storage

<!-- markdownlint-disable MD046 -->
!!! important

    This page is a work in progress. Please be patient as information is added or removed. Thank you.
<!-- markdownlint-enable MD046 -->

UAB Long-term storage (LTS) is an S3 object-storage platform hosted at UAB. This storage is designed to hold data that is not currently being used in analysis but should be kept for data sharing, recapitulation purposes, or reused for further analysis in the future. This documentation covers multiple methods for accessing LTS in Windows, Mac, and Linux environments.

<!-- markdownlint-disable MD046 -->
!!! important

    Currently, UAB LTS is only accessible using the campus network. If you are off campus and want to access LTS, please use the [UAB VPN](https://www.uab.edu/it/home/tech-solutions/network/vpn). If you are accessing LTS through Cheaha, you do not need to use the VPN even at home.

<!-- markdownlint-enable MD046 -->
## Terminology

When talking about S3 storage, some terms are different compared to a normal filesystem. This section is here to briefly explain some differences in case you go to other documentation and see these terms instead.

In S3, there are no such things as folders and files, just objects. Everything is an object, and there is no standard filesystem where things are physically stored in a heirarchical manner. However, there are certain things that can make it look that way such as prefixes. Prefixes are used in place of a file path to an object, and so can be used to represent a path to an object.

This documentation will use the standard file and path terms since that's more easily understood by most users. Just be aware that documentation such as [AWS CLI](https://awscli.amazonaws.com/v2/documentation/api/latest/index.html) will use terms prefix, object, and others that are not standard in a typical filesystem.

## Requesting an Account

UAB researchers do not have automatic access to LTS, and currently, single sign on is not enabled. To request access to LTS, please send an email to [support@listserv.uab.edu](support@listserv.uab.edu). You will be then be given an Access Key and a Secret Access Key, both of which will be used later on. Keep track of both of these keys and do not share them with anyone else, these are your login credentials for LTS.

## Windows/Mac

To access LTS from Windows and Mac, we suggest using the [Cyberduck](https://cyberduck.io/download/) GUI which is free to download.

Once you have it installed and open, Cyberduck will look like this:

![!Cyberduck basic interface](images/cyberduck.png)

### Creating a Connection

First, download the [UAB CyberDuck Connection Profile](UAB S3 Object Storage.cyberduckprofile). After it's downloaded, double click the file to open it in Cyberduck. It will open the following connection creation window:

![!Cyberduck UAB Connection Creation](images/cyberduck-open-connection.png)

Input your Access Key and Secret Access Key sent to you by Research Computing after account creation in their appropriate fields. Once you've entered these keys you can close the connection creation window. This connection with the keys you entered is now saved as a bookmark for easy access in the future. Double click the created bookmark to open the connection to LTS.

### Creating a Bucket

Sets of storage objects are stored in what are called buckets. Buckets are sets of file systems for storing data. You can create different buckets for various purposes, such as separating buckets by dataset or project, or having a single bucket for all data you are moving off of Cheaha project storage.

In order to create a bucket, click `File > New Folder...` and then name the bucket you would like to create. Once the bucket is created, it will appear in the File window. An example could look like:

![!Example bucket creation](images/cyberduck-create-bucket.png)

The bucket will have the symbol of a hard disk with an Amazon A brand on it. This is the root of the file system for that bucket. You can then double click into it to open that file system.

<!-- markdownlint-disable MD046 -->
!!! important

    Bucket names are shared across all LTS. This means you cannot create a bucket with a name that has already been created by someone else, even if that bucket is not shared with you. When creating bucket names, make them specific and/or unique. For example, davislab for storing data for the entire Davis lab or the name of a specific dataset that is being stored. Do not make names like trial or my-storage. 
<!-- markdownlint-enable MD046 -->

### Uploading and Downloading Data

Once you're inside the bucket, files can be uploaded easily through dragging and dropping from your local machine into the GUI. You can also use the `Upload` button in the toolbar to open a file browser and choose what to upload.

Downloading files from the bucket can be done by first selecting the file(s)/folder(s) to download and then clicking the `Actions` button in the toolbar. In that dropdown will be a `Download` option. You can also get to this dropdown through the `File` menu or by right-clicking.

<!-- markdownlint-disable MD046-->
!!! note

    By default, all buckets and files you upload are only available to you.Currently, there is not a known method to add permissions for individual researchers in the Cyberduck interface. If you need to add access to a bucket or set of files within a bucket, please look in the Linux guide below. If you do not have access to a personal Linux machine, Cheaha is available to use for this purpose.
<!-- markdownlint-enable MD046-->

### Alternative Interfaces

In addition to Cyberduck, there are other GUI based programs for interfacing with UAB LTS. [S3 Browser](https://s3browser.com/) is an easy-to-use program for uploading and downloading files. However more sophisticated tools, such as setting file permissions, are hidden behind a paywall. This tool is also only available on Windows platforms. researchers can investigate this tool if desired, however research computing will not provide direct support for this program.

## Linux/Command Line

Linux has very few workable GUIs capable of accessing S3 storage for free available, and so almost all tools for transferring from Cheaha to LTS will be command line interfaces (CLI). The positives for this are that CLIs offer a much broader range of function available to researchers for managing their LTS buckets.

There are a few different CLIs available to researchers on Cheaha to use. Current available CLIs on Cheaha are [rclone](https://rclone.org/) and the [AWS CLI](https://aws.amazon.com/cli/). This documentation will show how to perform each function using both tools where possible and will give a comparison chart contrasting what each tool is useful for.

Both of these are available as modules under the `rclone` and `awscli` module names.

<!-- markdownlint-disable MD046 -->
!!! note

    Of note, both rclone and AWS CLI are available for Windows and Mac as well if you are comfortable using command line interfaces on those platforms. There are installation instructions for both of these tools on their respective websites.
<!-- markdownlint-enable MD046 -->

### Configuration

In order to access LTS through the command line no matter which CLI you use, you will need to perform some configuration. This will set up the remote connection from your local machine (or you Cheaha profile) to LTS. If you choose to use multiple CLIs, you will need to perform configuration for each one separately.

#### rclone

The instructions for setting up a remote connection with rclone can be found in [our main rclone documentation](transfer/rclone.md#setting-up-an-s3-lts-remote). In the following examples, `uablts` is used as the name of the remote connection.

#### AWS CLI

Setting up a remote connection using AWS CLI is fairly straightforward. After loading the `awscli` module, run `aws configure` in the terminal.

It will ask you enter four pieces of information, fill them out as so:

``` text
AWS Access Key [none]: <access key>
AWS Secret Access Key [none]: <secret key>
Default region name [none]: <Press Enter>
Default output format [none]: json
```

Your access key and secret key should have been given to you by research computing when you requested your account. Copy-paste these into the terminal when requested.

<!-- markdownlint-disable MD046 -->
!!! important

    AWS CLI assumes you are using the AWS service for S3 storage. In order to access UAB LTS, for all AWS CLI commands, you will need to add the `--endpoint-url https://s3.lts.rc.uab.edu` option to the `aws` function call.
<!-- markdownlint-enable MD046 -->

### General Command Structure

**RClone**:

``` bash
rclone <subcommand> [options] <remote>:<bucket>
```

<!-- markdownlint-disable MD046 -->
!!! important

    For all commands, replace everything in `<>` with the necessary values. Do not include the `<>` symbols in your command.
<!-- markdownlint-enable MD046 -->

To see a list of all subcommands available to rclone, you can use `rclone --help`. You can also use the `--help` option with any subcommand to see a detailed explanation of what it does plus any options you may want or need to set when calling it.

**AWS CLI**:

``` bash
aws <command> <subcommand> [options and parameters]
```

The `<command>` for most commonly used functions will either be `s3` or `s3api`. You can use the `help` option to view available commands, subcommands, and options for AWS.

Additionally, when running basically any AWS CLI command, you can include the `--dryrun` option to see the exact actions that will be performed without actually performing them. This is useful for things like deleting files and folders to make sure you are not performing an unwanted action.

### Creating a Bucket

Buckets are essentially the root folder of a filesystem where you are storing your data. You will need to create a bucket before being able to copy data to LTS.

<!-- markdownlint-disable MD046 -->
!!! important

    Bucket names are shared across all LTS. This means you cannot create a bucket with a name that has already been created by someone else, even if that bucket is not shared with you. When creating bucket names, make them specific and/or unique. For example, davislab for storing data for the entire Davis lab or the name of a specific dataset that is being stored. Do not make names like trial or my-storage. 
<!-- markdownlint-enable MD046 -->

**RClone**:

``` bash
rclone mkdir uablts:<bucket>
```

**AWS CLI**:

``` bash
aws s3api create-bucket --bucket <bucketname> --endpoint-url https://s3.lts.rc.uab.edu
```

### Listing Buckets and Contents

**RClone**:

To list all buckets you have available, use the `lsd` subcommand with only the remote specified:

``` bash
rclone lsd uablts:
```

To list all contents inside a bucket, use the `ls` subcommand with the remote and bucket specified. You can also be specific about the path to the directory you want to list.

``` bash
rclone ls uablts:<bucket/path/to/directory>
```

This outputs all files along with their directory path recursively. So if you only specify the main bucket, it will output every file in the bucket no matter how deep in the directory tree.

To only list files and folders in a given directory, you can use the `lsf` subcommand

``` bash
rclone lsf uablts:<bucket/path/to/directory>
```

**AWS CLI**:

``` bash
aws s3 ls <bucket/path/to/directory/> --endpoint-url https://s3.lts.rc.uab.edu
```

If you would like to list all objects recursively, you can add the `--recursive` tag. A couple of other helpful options are `--summarize` and `--human-readable` that will give a total number of objects and their size and make the size output more easily readable, respectively.

<!-- markdownlint-disable MD046 -->
!!! important

    If you are wanting to look within a specific directory in S3, it is imperative to add the `/` at the end of the directory name when using `aws`.
<!-- markdownlint-enable MD046 -->

### Uploading Files and Folders

**RClone**:

Uploading files and folders can be done a couple of ways. The first is by using the `copy` subcommand. This will add files from the source to the destination.

``` bash
rclone copy <source> uablts:<bucket/destination>
```

The second method is using the `sync` subcommand. This subcommand makes the destination identical to the source. The `-i` option can be added to make it interactive, asking you whether to copy or delete each file.

``` bash
rclone sync [-i] <source> uablts:<bucket/destination>
```

<!-- markdownlint-disable MD046 -->
!!! danger

    Be extremely cautious using sync. If there are files in the destination that are not in the source, it will delete those files in addition to adding files to the destination. If data is deleted from LTS, it is not recoverable.
<!-- markdownlint-enable MD046 -->

**AWS CLI**:

Copying files and directories can be managed using the `cp` subcommand and has the same behavior as rclone's `copy`.

``` bash
aws s3 cp <source> s3://<bucketname/path/to/destination> --endpoint-url https://s3.lts.rc.uab.edu [--recursive]
```

If you are copying a directory, you will need to add the `--recursive` option. Additionally, the path to the bucket must include the `s3://` at the beginning unlike for other AWS commands. If you want to put the data you're copying into a folder that has not yet been created on the LTS system, you can just add that folder to the destination path and it will be automatically created.

If you are wanting to copy data down from LTS to your local machine or Cheaha, just reverse the positions of the source and destination in the function call.

Like rclone, AWS also has a sync subcommand that performs the same functionality. You can use it like so:

``` bash
aws s3 sync <source> s3://<bucketname/path/to/destination> --endpoint-url https://s3.lts.rc.uab.edu [--delete]
```

`sync` has an added benefit that only files that do not exist in the destination or files that have changed in the source will be transferred whereas `cp` copies everything no matter if it already exists in the destination. By default, `sync` DOES NOT cause files in the detination that are not in the source to be deleted like `rclone sync` does. If you want this functionality, you can add the `--delete` tag at the end of the function call.

<!-- markdownlint-disable MD046 -->
!!! danger

    Be extremely cautious using `--delete`. Only use if you are sure any data deleted is not important. Data deleted from LTS is not recoverable.
<!-- markdownlint-enable MD046 -->

### Deleting Files and Directories

**RClone**:

File deletion is performed using the `delete` subcommand.

``` bash
rclone delete uablts:<bucket/path/to/file>
```

Directory deletion is handled using the `purge` subcommand. Be very cautious with this, as this deletes all files and subdirectories within the directory as well.

``` bash
rclone purge uablts:<bucket/path/to/folder>
```

**AWS CLI**:

The subcommand for deleting files and folders from LTS is `rm`:

``` bash
aws s3 rm s3://<bucket/path/to/object> --endpoint-url https://s3.lts.rc.uab.edu [--recursive]
```

`rm` can delete both files and folders. If you are wanting to delete a folder and everything in it, you will need to add the `--recursive` option. The `s3://` tag is required at the beginning of the path as well, just like with the `cp` subcommand. Like with `sync` be very cautious using `rm` and make sure you know what you are deleting before you do so.

## Sharing Buckets

A major use we envision for LTS is storage of data that should be accessible to multiple users from a lab or research group. By default, buckets are only visible and accessible to the owner of the bucket, and no mechanism exists to search for buckets other users have created.

Instead, sharing buckets must be done through the command line using [bucket policies](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html). A bucket policy is a JSON formatted file that assigns user read and write permissions to the bucket and to specific objects within the bucket. If you have not worked with JSON files before, a brief explantion can be found [here](https://docs.fileformat.com/web/json/). An example policy file is shown below:

``` json
{
    "Version": "2022-10-05",
    "Statement": [{
        "Effect": "Allow",
        "Principal": {"AWS": ["arn:aws:iam:::user/mdefende"]},
        "Action": [
            "s3:ListBucket"
        ],
        "Resource": [
            "arn:aws:s3:::test-bucket"
        ]
    },
    {
        "Effect": "Allow",
        "Principal": {"AWS": ["arn:aws:iam:::user/mdefende"]},
        "Action": [
            "s3:GetObject"
        ],
        "Resource": [
            "arn:aws:s3:::test-bucket/*"
        ]
    }]
}
```
